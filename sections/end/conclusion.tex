\section{Conclusion and Outlook}
\label{sec:conclusion}

The study of \acp{SNN}, especially with regard to efficient learning procedures poses a interesting insight in the possibilities of modeling
systems on the human brain.
Concerning the difficulties of training a model which works with input values and their temporal dependencies researchers have already made 
impressive efforts.
The work of \cite{SNN} proposes an unsupervised approach with formidable performace in a variety of situations.
The authors of \cite{SNN} and \cite{Synaptic_plasticity} also emphasise the energy efficience of \acp{SNN} on neuromorphic hardware.

Besides this example of an unsupervised approach there is a variety of supervised methods to train a \ac{SNN}, 
for instance \ac{ANN}-\ac{SNN} conversion from \cite{DIET_SNN}.
Even though existing approaches perform well, the authors emphasise shortcomings and potential of further research.
\textcolor{red}{Bsp. offene Fragen}

Since synaptic plasticity dominates the computing costs of \acp{SNN} due to the fact that von Neumann architectures are not built for processing large number of small messages, i.e. spikes, 
other strategies to improve run-time efficiency rely on among others neuromorphic hardware \cite{simulation_STDP}.
This paper neglects the topic neuromorphic hardware.
Other work, such as \cite{Synaptic_plasticity}, 
cover several different implementations and offer a discussion about challenges (e.g. memory elements) and adchievements 
(e.g. successful synaptic rule implementations in hardware) in this area.
Yet another problem hindering the application of \acp{SNN} in real-world problems is accessibility of certain elements, such as individual synapses and neurons, 
to read and (re)configure the network topology \cite{hardware_STDP}.
The hardware seems to be in need of more improvements to be applicable to real-world problems.

wdhl wie gut es ist\\
was kommt noch?\\
wie zu verbessern?