\section{Introduction}

In order to find methods of computations requiring little power consumption, researchers have started creating structures modelled on the neurons of the brain.
The authors of \cite{SNN} found that \acs{SNN} were suitable means to tackle this task.
\acs{SNN} differ from normal \acs{ANN} in terms of their architecture and learning method.
Their inputs are 1-bit spike trains as opposed to 32- or 64-bit messages sent by \acs{ANN}.
Moreover, instead of backpropagation used for \acs{ANN}, \acs{SNN} have different learning rules to optimise their weight, such as \acfi{STDP} with exponential time dependence.
Regarding \cite{SNN}'s approach, \acfi{LIF} neurons and lateral inhibition are used.
Hence, the approach models the leaks of current of real neurons, as well as competition among the neurons.

Applications for this approach include pattern recognition presented in \cite{SNN} and object shape recognition from \cite{object_detection_SNN}.
The accuracy is surprisingly high for unsupervised methods.

This paper is structured as followed:
In \autoref{sec:main_part} the tackled problem regarding \acs{SNN}, context-specific terms, the network architecture, methods applied in the approach, as well as the approach itself are described.

\textcolor{red}{\autoref{sec:result}\\
\autoref{sec:comparison}\\
\autoref{sec:evaluation}\\
\autoref{sec:comparison}}