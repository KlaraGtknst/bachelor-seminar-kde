\section{Introduction}

In order to find methods of computation requiring little power consumption, 
researchers have started creating structures modelled on the neurons of the brain.
The authors of \cite{SNN} found that \acp{SNN} were suitable means to tackle this task.
\acp{SNN} differ from normal \acp{ANN} in terms of their architecture and learning method.
Their inputs are 1-bit spike trains as opposed to 32- or 64-bit messages of \acp{ANN}.
Moreover, instead of backpropagation used for \acp{ANN}, many \ac{SNN} models have different learning rules to optimise their weight, 
such as \acfi{STDP} with exponential time dependence.
The model uses \acfi{LIF} neurons and lateral inhibition \cite{SNN}.
Hence, the approach models the leak of current of real neurons, as well as competition among the neurons.

Applications for this approach include pattern recognition \cite{SNN} and object shape recognition \cite{object_detection_SNN}.
In both cases the accuracy is surprisingly high for an unsupervised method.

This paper is structured as followed:
In \autoref{sec:main_part} the tackled problem regarding \acp{SNN}, context-specific terms, the network architecture,
 as well as the approach itself are described.

\textcolor{red}{
In \autoref{sec:result} the results of tests approach is presented.
Different methods to train \acp{SNN} are described and compared in \autoref{sec:comparison}.
The paper concludes with a outlook in \autoref{sec:conclusion}.}