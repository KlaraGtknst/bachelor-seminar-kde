\section{Introduction}

In order to find methods of computation requiring little power consumption, 
researchers have started creating structures modelled on the neurons of the brain.
\acp{SNN} are allegedly suitable means to tackle this task \cite{SNN}.
They differ from normal \acp{ANN} in terms of their architecture and learning method.
Their inputs are 1-bit spike trains as opposed to 32- or 64-bit messages of \acp{ANN}.
\acp{SNN}' inputs are streams of events contrary to \acp{ANN}' inputs presentation at one time \cite{ANN_SNN_conversion}.
Moreover, instead of backpropagation used for \acp{ANN}, many \ac{SNN} models have different learning rules to optimise their weight, 
such as \acfi{STDP} with exponential time dependence.
The model uses \acfi{LIF} neurons and lateral inhibition \cite{SNN}.
Hence, the approach models the leak of current of real neurons, as well as competition among the neurons.

Applications for this approach include pattern recognition \cite{SNN} and object shape recognition \cite{object_detection_SNN}, \cite{multi_scale_STDP}.
In both cases the accuracy is surprisingly high for an unsupervised method.

This paper is structured as followed:
In \autoref{sec:main_part} the tackled problem regarding \acp{SNN}, context-specific terms, the network architecture,
 as well as the approach itself are described.
In \autoref{sec:result} the results of tests regarding performance, optimal parameter choice and other metrics of similar approaches are presented.
Methods to train \acp{SNN}, which differ more than the ones outlined in \autoref{sec:result} are described and compared in \autoref{sec:comparison}.
The paper concludes with a outlook in \autoref{sec:conclusion}.