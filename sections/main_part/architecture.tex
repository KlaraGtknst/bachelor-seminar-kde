\section{Network architecture}
\label{subsec:architecture}


The \ac{SNN} has an input layer and a \pLayer{} \cite{SNN}.
The input layer consists of $28 \times 28$ neurons, i.e. one for each input pixel.
All the neurons of the input layer are connected to all \eN{}s of the \pLayer{} (all-to-all).

The \pLayer{} has \eN{}s and \iN{}s.
A 1:1 excitatory to inhibitory neuron ratio is chosen, rather than the biologically plausible 4:1 ratio, 
to reduce computional complexity \cite{SNN}.
While every \eN{} is connected to exactly one \iN{} (one-to-one), every \iN{} is connected to all \eN{}s except the one it is already connected to.
The architecture is depicted in \autoref{fig:architecture_SNN}.
This structure creates lateral inhibition and creates competition among the \eN{}s.
\vspace{-4mm}
\begin{figure}[htbp]
    \center
    \includegraphics[width=0.5\textwidth]{pictures/fnins-15-638474-g001.jpg}
    \caption{Architecture of a \ac{SNN} from \authorsArchitectureSNNpicture{}'s work \cite{architecture_SNN_picture}.}
    \label{fig:architecture_SNN}
\end{figure}