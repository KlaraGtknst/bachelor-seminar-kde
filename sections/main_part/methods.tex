\subsection{Methods}
\label{subsec:methods}

Concepts used in \cite{SNN} include \textcolor{red}{\acfi{LIF} neurons}, \textcolor{red}{lateral inhibition}, \textcolor{red}{intrinsic plasticity} and \textcolor{red}{conductance-based synapses}.

\subsubsection{Timing of spikes}
The timing of spikes determines the type of synaptic changes \cite{LTP_D_bio}:
If multiple postsynaptic spikes occur close after presynaptic spikes a \acfi{LTP} is induced, 
whereas repetive postsynaptic spike before the presynaptic ones lead to \acfi{LTD}.


\subsubsection{synaptic plasticity}
Synapses play a crucial role in \ac{SNN} learning tasks such as recognition and computation \cite{Synaptic_plasticity}.
Synaptic plasticity rules is the term used for the different mathematical formulaes realised in activity-dependent modification of synaptic weights.
There is a great variety of synaptic plasticity rules, raging from abstract models to detailed models \cite{Synaptic_plasticity}.

Instances of abstract models include those based on the timing of spikes, such as Pair-based \ac{STDP} models.
\autoref{eq:pair_based_weight_update} from \cite{Synaptic_plasticity} shows the weight update of a pair-based \ac{STDP} model.
$\Delta t = t_{post} - t{pre}$ is the time difference between the presynaptic spike $t_{pre}$ and the postsynaptic spike $t_{post}$.
If a postsynaptic spike arrives with time window $\tau_+$ before a presynaptic one, potentiation occurs, i.e. the synaptic weight is increased. 
Analogously to induce depression the postsynaptic spike needs to precede the presynaptic one within time window $\tau_-$.
The amount of weight change depends on $\Delta t $ and the amplitude parameters $A^+$ and $A^-$.
%
\begin{equation}
    \centering
    \label{eq:pair_based_weight_update}
    \Delta w = \left\{
        \begin{array}{ll}
        \Delta w^+ = A^+ e^{\frac{-\Delta t}{\tau_+}} & \Delta t > 0 \\
        \Delta w^- = -A^- e^{\frac{\Delta t}{\tau_-}} & \Delta t \le 0 \\
        \end{array}
        \right.
\end{equation}
%
Other models, for instance the \ac{TSTDP}, not only take into account a single pair of presynaptic and postsynaptic spikes, 
but triplet combinations of spikes 
(current pre- and post-, current post- and previous post-, current pre- and previous presynaptic one) \cite{Synaptic_plasticity}.

Detailed models on the other hand, may take into account state variables, for instance membrane potential, 
accounting for more biophysically realistic models.
Changes in the synaptic weight of an instance of the \ac{SDSP} model depend on the postsynaptic membrane potential $V_{mem}$ and 
the calcium concentration $C(t)$ \cite{Synaptic_plasticity}.
Whenever a presynaptic spike arrives either potentiation of amount $a$ occurs, if the membrane potential $V_{mem}$ is higher than a certain threshold and the 
calcium concentration $C(t)$ is within certain bounds, or depression of amount $b$ analogously displayed in \autoref{eq:sdsp_weight_update}.
%
\begin{equation}
    \centering
    \label{eq:sdsp_weight_update} 
    W = 
    \left\{
    \begin{array}{ll}
        W + a, \text{if } V_{mem} > V_{mth} \text{ and } \Theta^l_{up} < C(t) < \theta^h_{up}\\
        W - b, \text{if } V_{mem} \le V_{mth} \text{ and } \Theta^l_{dn} < C(t) < \theta^h_{dn}
    \end{array}
    \right.
\end{equation}
%
The \ac{SDSP} approach models synaptic weight $W$ decay as displayed in \autoref{eq:sdsp_weight_decay} from \cite{Synaptic_plasticity}.
If the conditions from \autoref{eq:sdsp_weight_update} are no saisfied or no spike arrives, 
the synaptic weight drifts towards either high or low synaptic weight asymtotes dependent on the weights at a specific time $t$ 
with respect to a threshold $\theta_W$ \cite{Synaptic_plasticity}.
%
\begin{equation}
    \centering
    \label{eq:sdsp_weight_decay}
    \frac{dW(t)}{dt} = 
    \left\{
    \begin{array}{ll}
        \alpha, \text{if } W(t) > \theta_W\\
        - \beta, \text{if } W(t) \le \theta_W\\
    \end{array}
    \right.
\end{equation}
%
Other detailed models, such as \ac{LCP} are outlined in \cite{Synaptic_plasticity}.

\subsubsection{Learning using \ac{STDP}}
The synapses of the \ac{SNN} are trained using unsupervised \acfi{STDP}, which has been observed in a rage of species from insects to humans \cite{STDP_hebbian}. 
The weight of a connection between two neurons models a synapse.
\ac{STDP} is a synaptic learning rule, which adapts weights of synapses according to their degree of causality \cite{STDP_like},
 i.e. how likely the input causes postsynaptic neuron excitation.
\ac{STDP} increases synaptic weight if the postsynaptic neuron reacts immediately after the presynaptic neuron fires \cite{object_detection_SNN}.
The goal of \ac{STDP} is to strengthen synapses of pre- and postsynaptic neuron pairs, 
whose postsynaptic neuron reacts immediately after the presynaptic neuron fires \cite{object_detection_SNN}.
If the postsynaptic neuron fires before the presynaptic one the spike has another origin and thus, the synapse is weakened to disconnect the neurons.
%
\begin{equation}
    \centering
    \label{eq:weight_update}
    \Delta w = \eta (x_{pre} - x_{tar})(w_{max}-w)^\mu
\end{equation}
%
\autoref{eq:weight_update} from \cite{SNN} calculates the weight change after a \textcolor{red}{postsynaptic spike arrives 
(i.e. the synapses' importance is changed according to its influence on the postsynaptic neuron)}.
$\eta$ is the learning rate, presynaptic trace $x_{pre}$ tracks the number of recent presynaptic spikes 
(decaying if no spike arrives, increased by one otherwise), 
$\mu$ is the dependence of the update on the previous weight and 
$x_{tar}$ is the target value of the presynaptic trace at the moment of the postsynaptic spike.
If $x_{tar}$ is high, i.e. many spikes arrived at the postsynaptic neuron, the weight will possibly not be increased 
(especially if fewer spikes arrived at the presynaptic neuron indicated by  $x_{pre}$).

There are also other \ac{STDP} learning rules \cite{SNN}.
Some \ac{STDP} models rely on a teaching signal, which provides the right response \cite{STDP_like}.
Teaching signals are generated by Poison spike generators.
A teaching signal is sent to the correct pool of neurons representing the class (i.e. digit) after a stimulus was presented in the training phase.
The goal of this approach is to make each neuron pool selective to one class of input.


\subsubsection{Competitive Learning}
\textcolor{red}{position schlecht: lateral inhibition und neuron nicht erkl√§rt\\}
The goal of the \ac{SNN}-model is to train its neurons to represent prototypical inputs or an average of similar inputs \cite{SNN}.
To adchieve this goal, the weights of spiking neurons are adapted to become more similar to the input.
Lateral inhibition prevents too many neurons from spiking and thus, prevents them from becoming to similar in the course of adapting to the input.
This results in the receptive fields of the neurons explorating the input space.
To ensure that a approximately constant number of neurons' receptive fields is similiar to an input, 
homoeostasis guarantees similiar firing rates among the neurons.
The learning procedure is similar to k-means-like learning algorithms \cite{SNN}.
Hence, increasing the number of neurons may result in at most 95-97 \% accuracy.


\subsubsection{Neuron model}
\label{subsubsec:neuron_model}
In reality, a neuron fires if the membrane's potential crosses the membrane's threshold $\nu_{thresh}$.
After firing the neuron's membrane potential is reset and within the next few milliseconds cannot spike again \cite{SNN}.
%
\begin{equation}
    \centering
    \label{eq:membrane_vol_pot}
    \tau \frac{dV}{dt} = (E_{rest} - V) + g_e(E_{exc} - V) + g_i(E_{inh} - V)
\end{equation}
%
\autoref{eq:membrane_vol_pot} describes the membrane voltage 
(i.e. the value compared to the threshold $V_{thres}$ which determines whether the neuron fires) change over time constant $\tau$.
$E_{rest}$ is the resting membrane potential, $E_{exc}, E_{inh}$ are equilibrium potentials of excitatory and inhibitory synapses, 
and $g_e, g_i$ the conductances of excitatory and inhibitory synapses (i.e. the influence of respective synapses on membrane voltage of neuron) \cite{SNN}. 
The voltage decays (first parenthesis) and is influenced by the excitatory synapses 
adding to the voltage and the inhibitory synapses subtracting from the voltage.


\subsubsection{Synapse model}
\label{subsubsec:synapse_model}
The synapses' conductance $g_e$/$g_i$ ($e$ excitatory, $i$ inhibitory) model the influence of a presynaptic neuron on another neuron.
If a presynaptic spike arrives at the synapse the weight $w_{i,j}$ between neuron $i$ and neuron $j$ is added to $g_e$/$g_i$.
Otherwise, $g_e$/$g_i$ is decaying.
%
\begin{equation}
    \centering
    \label{eq:exc_conductance}
    \tau_{g_e} \frac{dg_e}{dt} = - g_e
\end{equation}
%
The decay is computed using \autoref{eq:exc_conductance} from \cite{SNN}.
The change over time constant $\tau$ is an \textcolor{red}{exponential decay}.


\subsubsection{\ac{LIF} neurons}
As depicted in \autoref{eq:membrane_vol_pot} from \autoref{subsubsec:neuron_model}, the membrane potential/ voltage $V$'s current leaks out of the neuron 
(i.e. decays without incoming spikes over time).
Due to the exponential decay, these neurons are called \ac{LIF} neurons.


\subsubsection{lateral inhibition}
Since every inhibitory neuron is connected to all excitatory neurons except the one it is already connected to, 
whenever a spike is triggered in an excitatory neuron all inhibitory neurons receive a spike as well.
Hence, neurons that did not fire are inhibited and thus, lateral inhibition and a soft winner-take-all mechanism are created.
An 1:1 excitatory to inhibitory neuron ratio is chosen, rather than the biologically plausible 4:1 ratio, 
to reduce computional complexity \cite{SNN}.





\subsubsection{conductance-based synapses}


\subsubsection{Homoeostasis}
In order to ensure the neurons have a similar firing rate, the \eN{}'s membrane threshold is calculated by $\nu_{thresh} + \theta$, 
where $\theta$ is increased every time the neuron fires and exponentially decaying otherwise.
Since the membrane potential is limited to $E_{exc}$ a neuron stops firing when its membrane threshold is higher than the membrane potential.

This technique countersteers the effect of inhomogeneity of the input and the lateral inhibition.


\subsubsection{Input encoding}
\acp{SNN} transmit information through spikes and thus, analog values have to be encoded into spikes \cite{DIET_SNN}.
There are different types of encodings based on certain beliefs, 
for instance those outlined in \autoref{subsubsec:communication}.
There is a method to encode analog values into spikes \cite{SNN}:
$60,000$ training examples, $10,000$ test examples of $28x28$ pixel images of the digits 0 to 9 compose the MNIST dataset used in \cite{SNN}.
Inputs are Poison spike trains, which are presented for 350ms.
The intensity of a pixel (0 to 255) is proportional to the firing rates (0 to $65.75 Hz$) of the neurons.
If the network does not react to the input, the maximum input firing rate is augmented until it fires in the desired fashion. 


\subsubsection{Training}
In order to allow all variables to decay there is 150ms phase without any input between images.

\subsubsection{Testing}
First the learning rate $\eta$ is set to zero to appoint the neuron's threshold.
After that the training set is presented once more.
The highest response among the ten digit classes is used to assign a class to each neuron, i.e. labels are used.
The predicted value for an input is the class whose neurons have the highest average firing rate.
The classification accuracy is determined on the MNIST test set.